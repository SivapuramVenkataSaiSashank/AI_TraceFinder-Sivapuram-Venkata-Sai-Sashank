{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f4f873",
   "metadata": {},
   "source": [
    "# ðŸ“’ Scanner ID Pipeline with Flatfield Fingerprints\n",
    "This notebook extracts **scanner fingerprints** from flat-field images, then compares document noise patterns against them using correlation, FFT features, and trains ML models (SVM/CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "from skimage import io, color\n",
    "from skimage.util import img_as_float32\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "from numpy.fft import fft2, fftshift\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EDIT THESE PATHS FOR YOUR MACHINE ====\n",
    "FLATROOT = r\"D:\\scanner_id_pipeline\\data\\flatfields\"\n",
    "OFFICIALROOT = r\"D:\\scanner_id_pipeline\\data\\Official\"\n",
    "WIKIROOT = r\"D:\\scanner_id_pipeline\\data\\Wikipedia\"\n",
    "\n",
    "def check_path(p):\n",
    "    print(p, \"=>\", \"OK\" if os.path.exists(p) else \"MISSING\")\n",
    "\n",
    "print(\"Checking your paths...\")\n",
    "check_path(FLATROOT)\n",
    "check_path(OFFICIALROOT)\n",
    "check_path(WIKIROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_gray(path: str) -> np.ndarray:\n",
    "    img = io.imread(path)\n",
    "    if img.ndim == 3:\n",
    "        if img.shape[-1] == 4:\n",
    "            img = img[..., :3]\n",
    "        img = color.rgb2gray(img)\n",
    "    else:\n",
    "        img = img.astype(np.float32) / (np.iinfo(img.dtype).max if np.issubdtype(img.dtype, np.integer) else 1.0)\n",
    "    return img_as_float32(img)\n",
    "\n",
    "def normalize_image(img: np.ndarray) -> np.ndarray:\n",
    "    m, s = np.mean(img), np.std(img) + 1e-8\n",
    "    return (img - m) / s\n",
    "\n",
    "def residual_wavelet(img: np.ndarray) -> np.ndarray:\n",
    "    den = denoise_wavelet(img, method='BayesShrink', mode='soft', rescale_sigma=True)\n",
    "    res = img - den\n",
    "    return res.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e21144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_images(root: str, exts=('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')) -> List[str]:\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(glob.glob(os.path.join(root, '**', f'*{ext}'), recursive=True))\n",
    "    return sorted(files)\n",
    "\n",
    "def group_by_scanner(root: str) -> Dict[str, List[str]]:\n",
    "    paths = list_images(root)\n",
    "    mapping = {}\n",
    "    for p in paths:\n",
    "        rel = os.path.relpath(p, root)\n",
    "        scanner = rel.split(os.sep)[0]\n",
    "        mapping.setdefault(scanner, []).append(p)\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc72bb0",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Step 1: Build Flatfield Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scanner_fingerprints(flatroot: str) -> Dict[str, np.ndarray]:\n",
    "    by_scanner = group_by_scanner(flatroot)\n",
    "    fingerprints = {}\n",
    "    for lab, paths in by_scanner.items():\n",
    "        resids = []\n",
    "        for p in paths:\n",
    "            img = load_image_gray(p)\n",
    "            img = normalize_image(img)\n",
    "            res = residual_wavelet(img)\n",
    "            resids.append(res)\n",
    "        if resids:\n",
    "            fingerprints[lab] = np.mean(resids, axis=0).astype(np.float32)\n",
    "            print(f\"Built fingerprint for {lab}, using {len(resids)} flatfield images.\")\n",
    "    return fingerprints\n",
    "\n",
    "fingerprints = build_scanner_fingerprints(FLATROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1aa00",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Step 2: Extract Document Residuals and Correlate with Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a829e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2(a, b):\n",
    "    a, b = a - np.mean(a), b - np.mean(b)\n",
    "    return np.sum(a*b) / (np.sqrt(np.sum(a*a)) * np.sqrt(np.sum(b*b)) + 1e-8)\n",
    "\n",
    "def correlation_features(residual: np.ndarray, fingerprints: Dict[str, np.ndarray], label_names: List[str]) -> np.ndarray:\n",
    "    return np.array([corr2(residual, fingerprints[lab]) for lab in label_names], dtype=np.float32)\n",
    "\n",
    "def fft_radial_stats(patch: np.ndarray, n_bins: int = 16) -> np.ndarray:\n",
    "    F = fftshift(fft2(patch))\n",
    "    P = np.abs(F) ** 2\n",
    "    H, W = P.shape\n",
    "    cy, cx = H//2, W//2\n",
    "    y, x = np.indices(P.shape)\n",
    "    r = np.sqrt((y - cy)**2 + (x - cx)**2)\n",
    "    r_norm = r / (r.max() + 1e-8)\n",
    "    bins = np.linspace(0, 1.0, n_bins+1)\n",
    "    feats = []\n",
    "    for i in range(n_bins):\n",
    "        mask = (r_norm >= bins[i]) & (r_norm < bins[i+1])\n",
    "        feats.append(P[mask].mean() if np.any(mask) else 0.0)\n",
    "    feats = np.log1p(np.array(feats, dtype=np.float32))\n",
    "    return (feats - feats.mean()) / (feats.std() + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040eb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_document_dataset(docroot: str, fingerprints: Dict[str, np.ndarray], max_docs: int = None):\n",
    "    by_scanner = group_by_scanner(docroot)\n",
    "    label_names = sorted(fingerprints.keys())\n",
    "    X_list, y_list = [], []\n",
    "    for lab in by_scanner.keys():\n",
    "        for p in by_scanner[lab][:max_docs or None]:\n",
    "            img = load_image_gray(p)\n",
    "            img = normalize_image(img)\n",
    "            res = residual_wavelet(img)\n",
    "            # feature = correlation with fingerprints + FFT features\n",
    "            feats = np.concatenate([correlation_features(res, fingerprints, label_names),\n",
    "                                    fft_radial_stats(res)], axis=0)\n",
    "            X_list.append(feats)\n",
    "            y_list.append(label_names.index(lab))\n",
    "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64), label_names\n",
    "\n",
    "X_off, y_off, labels = build_document_dataset(OFFICIALROOT, fingerprints)\n",
    "X_wiki, y_wiki, _ = build_document_dataset(WIKIROOT, fingerprints)\n",
    "\n",
    "X = np.concatenate([X_off, X_wiki], axis=0)\n",
    "y = np.concatenate([y_off, y_wiki], axis=0)\n",
    "print(\"Dataset:\", X.shape, \"labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f64583",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Step 3: Train ML Classifier on Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_svm_pipeline(C: float = 10.0, gamma: str | float = 'scale') -> Pipeline:\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SVC(C=C, kernel='rbf', gamma=gamma, probability=True, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "def train_and_eval(X, y, labels, test_size=0.25):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "    model = make_svm_pipeline()\n",
    "    model.fit(Xtr, ytr)\n",
    "    yhat_te = model.predict(Xte)\n",
    "    print(\"Test Accuracy:\", accuracy_score(yte, yhat_te))\n",
    "    print(classification_report(yte, yhat_te, target_names=labels))\n",
    "    cm = confusion_matrix(yte, yhat_te)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=labels).plot(xticks_rotation=45)\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "svm_model = train_and_eval(X, y, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7a537",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Step 4: Train with Random Forest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_random_forest(X, y, labels, test_size=0.25):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "    model.fit(Xtr, ytr)\n",
    "    yhat = model.predict(Xte)\n",
    "    print(\"Random Forest Test Accuracy:\", accuracy_score(yte, yhat))\n",
    "    print(classification_report(yte, yhat, target_names=labels))\n",
    "    return model\n",
    "\n",
    "def train_xgboost(X, y, labels, test_size=0.25):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "        objective='multi:softmax', num_class=len(labels), random_state=42, n_jobs=-1\n",
    "    )\n",
    "    model.fit(Xtr, ytr)\n",
    "    yhat = model.predict(Xte)\n",
    "    print(\"XGBoost Test Accuracy:\", accuracy_score(yte, yhat))\n",
    "    print(classification_report(yte, yhat, target_names=labels))\n",
    "    return model\n",
    "\n",
    "rf_model = train_random_forest(X, y, labels)\n",
    "xgb_model = train_xgboost(X, y, labels)\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump({'model': rf_model, 'labels': labels}, \"artifacts/random_forest.joblib\")\n",
    "joblib.dump({'model': xgb_model, 'labels': labels}, \"artifacts/xgboost.joblib\")\n",
    "print(\"Saved Random Forest and XGBoost models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aeb577",
   "metadata": {},
   "source": [
    "ðŸ”‘Step 5: CNN Classifier (ResNet18 on Residual Patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1383451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# ---- Dataset wrapper for patches ----\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, patches: np.ndarray, labels: np.ndarray):\n",
    "        self.X = patches.astype(np.float32)\n",
    "        self.y = labels.astype(np.int64)\n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][None, ...]), torch.tensor(self.y[idx])\n",
    "\n",
    "# ---- ResNet18 modified for grayscale ----\n",
    "class ResNetScanner(nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        in_feats = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_feats, n_classes)\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# ---- Patch extraction ----\n",
    "def extract_patches(img: np.ndarray, patch: int = 128, stride: int = 128, min_margin: int = 16):\n",
    "    H, W = img.shape\n",
    "    patches = []\n",
    "    for y in range(min_margin, H - patch - min_margin + 1, stride):\n",
    "        for x in range(min_margin, W - patch - min_margin + 1, stride):\n",
    "            patches.append(img[y:y+patch, x:x+patch])\n",
    "    return np.stack(patches, axis=0) if patches else np.empty((0, patch, patch), dtype=img.dtype)\n",
    "\n",
    "def build_patch_dataset(docroot: str, max_docs: int = None):\n",
    "    by_scanner = group_by_scanner(docroot)\n",
    "    X_list, y_list, labels = [], [], sorted(by_scanner.keys())\n",
    "    for lab in labels:\n",
    "        for p in by_scanner[lab][:max_docs or None]:\n",
    "            img = load_image_gray(p)\n",
    "            img = normalize_image(img)\n",
    "            res = residual_wavelet(img)\n",
    "            patches = extract_patches(res, patch=128, stride=256)\n",
    "            for ph in patches:\n",
    "                X_list.append(ph)\n",
    "                y_list.append(labels.index(lab))\n",
    "    return np.stack(X_list), np.array(y_list), labels\n",
    "\n",
    "# Build patch dataset from both sources\n",
    "X_patches_off, y_patches_off, _ = build_patch_dataset(OFFICIALROOT)\n",
    "X_patches_wiki, y_patches_wiki, _ = build_patch_dataset(WIKIROOT)\n",
    "X_patches = np.concatenate([X_patches_off, X_patches_wiki], axis=0)\n",
    "y_patches = np.concatenate([y_patches_off, y_patches_wiki], axis=0)\n",
    "print(\"Patch dataset:\", X_patches.shape, \"labels:\", len(set(y_patches)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddf149",
   "metadata": {},
   "source": [
    "ðŸ”‘ Step 6: Train ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc128db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(patches, labels, epochs=10, batch=64, lr=1e-4):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ds = PatchDataset(patches, labels)\n",
    "    n_val = max(1, int(0.2 * len(ds)))\n",
    "    n_train = len(ds) - n_val\n",
    "    train_ds, val_ds = random_split(ds, [n_train, n_val])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch)\n",
    "\n",
    "    model = ResNetScanner(int(labels.max()+1)).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb).argmax(1)\n",
    "                correct += (pred == yb).sum().item()\n",
    "                total += len(yb)\n",
    "        print(f\"Epoch {ep+1}/{epochs}, Val Acc: {correct/max(1,total):.3f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "cnn_model = train_resnet(X_patches, y_patches, epochs=10, batch=64, lr=1e-4)\n",
    "\n",
    "torch.save({'state_dict': cnn_model.state_dict()}, \"artifacts/cnn_resnet18.pt\")\n",
    "print(\"Saved CNN model to artifacts/cnn_resnet18.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64755ffb",
   "metadata": {},
   "source": [
    "ðŸ”‘ Step 7: Final CNN Evaluation (Overall Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d133c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def evaluate_resnet(model, patches, labels):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ds = PatchDataset(patches, labels)\n",
    "    dl = DataLoader(ds, batch_size=64)\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            out = model(xb).argmax(1).cpu().numpy()\n",
    "            preds.extend(out)\n",
    "            trues.extend(yb.numpy())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    print(\"Final CNN Overall Accuracy:\", acc)\n",
    "    print(classification_report(trues, preds))\n",
    "    return acc\n",
    "\n",
    "# Evaluate on all patches used\n",
    "evaluate_resnet(cnn_model, X_patches, y_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf4537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8506302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bd9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843f40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa888d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Scanner ID Pipeline with Flatfield Fingerprints - IMPROVED VERSION\n",
    "# This notebook extracts scanner fingerprints from flat-field images, then compares \n",
    "# document noise patterns against them using correlation, FFT features, and trains ML models (SVM/CNN).\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "# Image processing\n",
    "from skimage import io, color\n",
    "from skimage.util import img_as_float32\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "# FFT\n",
    "from numpy.fft import fft2, fftshift\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"Warning: XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the pipeline\"\"\"\n",
    "    # EDIT THESE PATHS FOR YOUR MACHINE\n",
    "    FLATROOT = r\"D:\\scanner_id_pipeline\\data\\flatfields\"\n",
    "    OFFICIALROOT = r\"D:\\scanner_id_pipeline\\data\\Official\"\n",
    "    WIKIROOT = r\"D:\\scanner_id_pipeline\\data\\Wikipedia\"\n",
    "    ARTIFACTS_DIR = \"artifacts\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    MAX_DOCS_PER_SCANNER = None  # None for all documents\n",
    "    PATCH_SIZE = 128\n",
    "    PATCH_STRIDE = 256\n",
    "    MIN_MARGIN = 16\n",
    "    FFT_BINS = 16\n",
    "    \n",
    "    # ML parameters\n",
    "    TEST_SIZE = 0.25\n",
    "    RANDOM_STATE = 42\n",
    "    CNN_EPOCHS = 15  # Increased from 10\n",
    "    CNN_BATCH_SIZE = 64\n",
    "    CNN_LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # Memory management\n",
    "    MAX_PATCHES_IN_MEMORY = 10000  # Limit patches to prevent memory issues\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def check_paths():\n",
    "    \"\"\"Check if all required paths exist\"\"\"\n",
    "    paths = {\n",
    "        \"Flatfields\": config.FLATROOT,\n",
    "        \"Official Documents\": config.OFFICIALROOT, \n",
    "        \"Wikipedia Documents\": config.WIKIROOT\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Checking paths...\")\n",
    "    all_exist = True\n",
    "    for name, path in paths.items():\n",
    "        exists = os.path.exists(path)\n",
    "        status = \"âœ“ OK\" if exists else \"âœ— MISSING\"\n",
    "        logger.info(f\"{name}: {path} => {status}\")\n",
    "        if not exists:\n",
    "            all_exist = False\n",
    "    \n",
    "    if not all_exist:\n",
    "        raise FileNotFoundError(\"Some required paths are missing. Please update Config class.\")\n",
    "    \n",
    "    # Create artifacts directory\n",
    "    Path(config.ARTIFACTS_DIR).mkdir(exist_ok=True)\n",
    "    logger.info(f\"Artifacts directory: {config.ARTIFACTS_DIR}\")\n",
    "\n",
    "# ==== IMAGE PROCESSING FUNCTIONS ====\n",
    "def load_image_gray(path: str) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load and convert image to grayscale with error handling\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            logger.warning(f\"File not found: {path}\")\n",
    "            return None\n",
    "            \n",
    "        img = io.imread(path)\n",
    "        \n",
    "        # Handle different image formats\n",
    "        if img.ndim == 3:\n",
    "            if img.shape[-1] == 4:  # RGBA\n",
    "                img = img[..., :3]  # Remove alpha channel\n",
    "            img = color.rgb2gray(img)\n",
    "        elif img.ndim == 2:\n",
    "            # Already grayscale\n",
    "            if np.issubdtype(img.dtype, np.integer):\n",
    "                img = img.astype(np.float32) / np.iinfo(img.dtype).max\n",
    "            else:\n",
    "                img = img.astype(np.float32)\n",
    "        else:\n",
    "            logger.warning(f\"Unexpected image dimensions: {img.shape} for {path}\")\n",
    "            return None\n",
    "            \n",
    "        return img_as_float32(img)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load image {path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def normalize_image(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize image to zero mean and unit variance\"\"\"\n",
    "    if img is None:\n",
    "        return None\n",
    "    mean_val = np.mean(img)\n",
    "    std_val = np.std(img)\n",
    "    if std_val < 1e-8:\n",
    "        logger.warning(\"Image has very low variance, normalization might be unstable\")\n",
    "        std_val = 1e-8\n",
    "    return (img - mean_val) / std_val\n",
    "\n",
    "def residual_wavelet(img: np.ndarray) -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract wavelet residual with error handling\"\"\"\n",
    "    try:\n",
    "        if img is None:\n",
    "            return None\n",
    "        denoised = denoise_wavelet(img, method='BayesShrink', mode='soft', rescale_sigma=True)\n",
    "        residual = img - denoised\n",
    "        return residual.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Wavelet denoising failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ==== FILE DISCOVERY FUNCTIONS ====\n",
    "def list_images(root: str, extensions=('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')) -> List[str]:\n",
    "    \"\"\"List all image files recursively with better error handling\"\"\"\n",
    "    if not os.path.exists(root):\n",
    "        logger.warning(f\"Directory does not exist: {root}\")\n",
    "        return []\n",
    "        \n",
    "    files = []\n",
    "    try:\n",
    "        for ext in extensions:\n",
    "            # Case insensitive search\n",
    "            pattern = os.path.join(root, '**', f'*{ext}')\n",
    "            files.extend(glob.glob(pattern, recursive=True))\n",
    "            # Also search uppercase\n",
    "            pattern_upper = os.path.join(root, '**', f'*{ext.upper()}')\n",
    "            files.extend(glob.glob(pattern_upper, recursive=True))\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        files = sorted(list(set(files)))\n",
    "        logger.info(f\"Found {len(files)} image files in {root}\")\n",
    "        return files\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error listing images in {root}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def group_by_scanner(root: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Group image paths by scanner (first directory level)\"\"\"\n",
    "    paths = list_images(root)\n",
    "    mapping = {}\n",
    "    \n",
    "    for path in paths:\n",
    "        try:\n",
    "            rel_path = os.path.relpath(path, root)\n",
    "            scanner_name = rel_path.split(os.sep)[0]\n",
    "            mapping.setdefault(scanner_name, []).append(path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not process path {path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Log scanner distribution\n",
    "    for scanner, files in mapping.items():\n",
    "        logger.info(f\"Scanner '{scanner}': {len(files)} files\")\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# ==== FINGERPRINT BUILDING ====\n",
    "def build_scanner_fingerprints(flatroot: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Build scanner fingerprints from flatfield images with improved error handling\"\"\"\n",
    "    by_scanner = group_by_scanner(flatroot)\n",
    "    fingerprints = {}\n",
    "    \n",
    "    for scanner_name, paths in by_scanner.items():\n",
    "        logger.info(f\"Processing scanner: {scanner_name}\")\n",
    "        residuals = []\n",
    "        failed_count = 0\n",
    "        \n",
    "        for path in paths:\n",
    "            img = load_image_gray(path)\n",
    "            if img is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "                \n",
    "            img_norm = normalize_image(img)\n",
    "            if img_norm is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "                \n",
    "            residual = residual_wavelet(img_norm)\n",
    "            if residual is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "                \n",
    "            residuals.append(residual)\n",
    "        \n",
    "        if residuals:\n",
    "            # Ensure all residuals have the same shape\n",
    "            shapes = [r.shape for r in residuals]\n",
    "            if len(set(shapes)) > 1:\n",
    "                logger.warning(f\"Scanner {scanner_name} has images with different shapes: {set(shapes)}\")\n",
    "                # Find most common shape\n",
    "                common_shape = Counter(shapes).most_common(1)[0][0]\n",
    "                residuals = [r for r in residuals if r.shape == common_shape]\n",
    "                logger.info(f\"Using {len(residuals)} images with shape {common_shape}\")\n",
    "            \n",
    "            if residuals:\n",
    "                fingerprint = np.mean(residuals, axis=0).astype(np.float32)\n",
    "                fingerprints[scanner_name] = fingerprint\n",
    "                logger.info(f\"Built fingerprint for {scanner_name} using {len(residuals)} images\")\n",
    "                if failed_count > 0:\n",
    "                    logger.warning(f\"Failed to process {failed_count} images for {scanner_name}\")\n",
    "            else:\n",
    "                logger.error(f\"No valid residuals for scanner {scanner_name}\")\n",
    "        else:\n",
    "            logger.error(f\"No flatfield images found for scanner {scanner_name}\")\n",
    "    \n",
    "    return fingerprints\n",
    "\n",
    "# ==== FEATURE EXTRACTION ====\n",
    "def correlation_coefficient(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Compute normalized correlation coefficient with numerical stability\"\"\"\n",
    "    try:\n",
    "        a_centered = a - np.mean(a)\n",
    "        b_centered = b - np.mean(b)\n",
    "        \n",
    "        numerator = np.sum(a_centered * b_centered)\n",
    "        denominator = np.sqrt(np.sum(a_centered**2)) * np.sqrt(np.sum(b_centered**2))\n",
    "        \n",
    "        if denominator < 1e-10:\n",
    "            return 0.0\n",
    "        \n",
    "        return float(numerator / denominator)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def correlation_features(residual: np.ndarray, fingerprints: Dict[str, np.ndarray], \n",
    "                        label_names: List[str]) -> np.ndarray:\n",
    "    \"\"\"Compute correlation features with all scanner fingerprints\"\"\"\n",
    "    features = []\n",
    "    for label in label_names:\n",
    "        if label in fingerprints:\n",
    "            # Ensure shapes match\n",
    "            if residual.shape != fingerprints[label].shape:\n",
    "                # Resize to minimum common shape\n",
    "                min_h = min(residual.shape[0], fingerprints[label].shape[0])\n",
    "                min_w = min(residual.shape[1], fingerprints[label].shape[1])\n",
    "                res_crop = residual[:min_h, :min_w]\n",
    "                fp_crop = fingerprints[label][:min_h, :min_w]\n",
    "                corr = correlation_coefficient(res_crop, fp_crop)\n",
    "            else:\n",
    "                corr = correlation_coefficient(residual, fingerprints[label])\n",
    "            features.append(corr)\n",
    "        else:\n",
    "            features.append(0.0)\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def fft_radial_stats(patch: np.ndarray, n_bins: int = None) -> np.ndarray:\n",
    "    \"\"\"Compute radial FFT statistics with improved numerical stability\"\"\"\n",
    "    if n_bins is None:\n",
    "        n_bins = config.FFT_BINS\n",
    "        \n",
    "    try:\n",
    "        # Apply window to reduce spectral leakage\n",
    "        window = np.outer(np.hanning(patch.shape[0]), np.hanning(patch.shape[1]))\n",
    "        windowed_patch = patch * window\n",
    "        \n",
    "        F = fftshift(fft2(windowed_patch))\n",
    "        power_spectrum = np.abs(F) ** 2\n",
    "        \n",
    "        H, W = power_spectrum.shape\n",
    "        center_y, center_x = H // 2, W // 2\n",
    "        \n",
    "        y, x = np.indices(power_spectrum.shape)\n",
    "        radius = np.sqrt((y - center_y)**2 + (x - center_x)**2)\n",
    "        max_radius = np.sqrt(center_y**2 + center_x**2)\n",
    "        \n",
    "        if max_radius < 1e-6:\n",
    "            return np.zeros(n_bins, dtype=np.float32)\n",
    "        \n",
    "        radius_normalized = radius / max_radius\n",
    "        bins = np.linspace(0, 1.0, n_bins + 1)\n",
    "        \n",
    "        features = []\n",
    "        for i in range(n_bins):\n",
    "            mask = (radius_normalized >= bins[i]) & (radius_normalized < bins[i + 1])\n",
    "            if np.any(mask):\n",
    "                mean_power = np.mean(power_spectrum[mask])\n",
    "                features.append(np.log1p(mean_power))  # log(1 + x) for numerical stability\n",
    "            else:\n",
    "                features.append(0.0)\n",
    "        \n",
    "        features = np.array(features, dtype=np.float32)\n",
    "        \n",
    "        # Normalize features\n",
    "        mean_feat = np.mean(features)\n",
    "        std_feat = np.std(features)\n",
    "        if std_feat > 1e-8:\n",
    "            features = (features - mean_feat) / std_feat\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"FFT feature extraction failed: {str(e)}\")\n",
    "        return np.zeros(n_bins, dtype=np.float32)\n",
    "\n",
    "# ==== DATASET BUILDING ====\n",
    "def build_document_dataset(docroot: str, fingerprints: Dict[str, np.ndarray], \n",
    "                          max_docs: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"Build dataset from document images with memory management\"\"\"\n",
    "    by_scanner = group_by_scanner(docroot)\n",
    "    label_names = sorted(fingerprints.keys())\n",
    "    \n",
    "    # Filter to only scanners that have fingerprints\n",
    "    by_scanner = {k: v for k, v in by_scanner.items() if k in fingerprints}\n",
    "    \n",
    "    if not by_scanner:\n",
    "        logger.error(\"No scanners found with matching fingerprints!\")\n",
    "        return np.array([]), np.array([]), []\n",
    "    \n",
    "    X_list, y_list = [], []\n",
    "    total_processed = 0\n",
    "    \n",
    "    for scanner_name in by_scanner.keys():\n",
    "        files_to_process = by_scanner[scanner_name][:max_docs] if max_docs else by_scanner[scanner_name]\n",
    "        \n",
    "        logger.info(f\"Processing {len(files_to_process)} documents for scanner {scanner_name}\")\n",
    "        \n",
    "        for i, path in enumerate(files_to_process):\n",
    "            if i % 50 == 0:  # Progress logging\n",
    "                logger.info(f\"  Progress: {i}/{len(files_to_process)}\")\n",
    "            \n",
    "            img = load_image_gray(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            img_norm = normalize_image(img)\n",
    "            if img_norm is None:\n",
    "                continue\n",
    "                \n",
    "            residual = residual_wavelet(img_norm)\n",
    "            if residual is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract features\n",
    "            corr_feats = correlation_features(residual, fingerprints, label_names)\n",
    "            fft_feats = fft_radial_stats(residual)\n",
    "            \n",
    "            # Combine features\n",
    "            combined_features = np.concatenate([corr_feats, fft_feats])\n",
    "            \n",
    "            X_list.append(combined_features)\n",
    "            y_list.append(label_names.index(scanner_name))\n",
    "            total_processed += 1\n",
    "    \n",
    "    if not X_list:\n",
    "        logger.error(\"No valid features extracted!\")\n",
    "        return np.array([]), np.array([]), []\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "    \n",
    "    logger.info(f\"Dataset built: {X.shape[0]} samples, {X.shape[1]} features, {len(label_names)} classes\")\n",
    "    return X, y, label_names\n",
    "\n",
    "# ==== MACHINE LEARNING MODELS ====\n",
    "def create_svm_pipeline(C: float = 10.0, gamma: Union[str, float] = 'scale') -> Pipeline:\n",
    "    \"\"\"Create SVM pipeline with preprocessing\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(C=C, kernel='rbf', gamma=gamma, probability=True, \n",
    "                   class_weight='balanced', random_state=config.RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "def train_and_evaluate_model(X: np.ndarray, y: np.ndarray, labels: List[str], \n",
    "                           model_name: str, model, test_size: float = None) -> object:\n",
    "    \"\"\"Generic function to train and evaluate models with cross-validation\"\"\"\n",
    "    if test_size is None:\n",
    "        test_size = config.TEST_SIZE\n",
    "    \n",
    "    logger.info(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Stratified split to ensure balanced classes\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=config.RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    logger.info(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\n=== {model_name} Results ===\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=labels))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ConfusionMatrixDisplay(cm, display_labels=labels).plot(xticks_rotation=45)\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_random_forest(X: np.ndarray, y: np.ndarray, labels: List[str]) -> RandomForestClassifier:\n",
    "    \"\"\"Train Random Forest classifier\"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=config.RANDOM_STATE, \n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    return train_and_evaluate_model(X, y, labels, \"Random Forest\", model)\n",
    "\n",
    "def train_xgboost(X: np.ndarray, y: np.ndarray, labels: List[str]) -> object:\n",
    "    \"\"\"Train XGBoost classifier if available\"\"\"\n",
    "    if not XGBOOST_AVAILABLE:\n",
    "        logger.warning(\"XGBoost not available, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(labels),\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    return train_and_evaluate_model(X, y, labels, \"XGBoost\", model)\n",
    "\n",
    "# ==== DEEP LEARNING COMPONENTS ====\n",
    "class PatchDataset(Dataset):\n",
    "    \"\"\"Dataset class for CNN training with patches\"\"\"\n",
    "    def __init__(self, patches: np.ndarray, labels: np.ndarray):\n",
    "        self.patches = patches.astype(np.float32)\n",
    "        self.labels = labels.astype(np.int64)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Add channel dimension for grayscale\n",
    "        patch = torch.from_numpy(self.patches[idx][None, ...])  # Shape: (1, H, W)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return patch, label\n",
    "\n",
    "class ResNetScanner(nn.Module):\n",
    "    \"\"\"ResNet18 modified for grayscale scanner identification\"\"\"\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Modify first conv layer for grayscale input\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Modify final layer\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, n_classes)\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.resnet.avgpool(self.resnet.layer4(\n",
    "            self.resnet.layer3(\n",
    "                self.resnet.layer2(\n",
    "                    self.resnet.layer1(\n",
    "                        self.resnet.relu(\n",
    "                            self.resnet.bn1(\n",
    "                                self.resnet.conv1(x)\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ))\n",
    "        features = torch.flatten(features, 1)\n",
    "        features = self.dropout(features)\n",
    "        return self.resnet.fc(features)\n",
    "\n",
    "def extract_patches_from_image(img: np.ndarray, patch_size: int = None, \n",
    "                              stride: int = None, min_margin: int = None) -> np.ndarray:\n",
    "    \"\"\"Extract patches from image with memory-efficient approach\"\"\"\n",
    "    if patch_size is None:\n",
    "        patch_size = config.PATCH_SIZE\n",
    "    if stride is None:\n",
    "        stride = config.PATCH_STRIDE\n",
    "    if min_margin is None:\n",
    "        min_margin = config.MIN_MARGIN\n",
    "    \n",
    "    H, W = img.shape\n",
    "    patches = []\n",
    "    \n",
    "    # Calculate number of patches to avoid memory issues\n",
    "    max_patches_per_image = config.MAX_PATCHES_IN_MEMORY // 100  # Conservative estimate\n",
    "    \n",
    "    y_positions = list(range(min_margin, H - patch_size - min_margin + 1, stride))\n",
    "    x_positions = list(range(min_margin, W - patch_size - min_margin + 1, stride))\n",
    "    \n",
    "    # Limit number of patches if too many\n",
    "    total_possible = len(y_positions) * len(x_positions)\n",
    "    if total_possible > max_patches_per_image:\n",
    "        # Randomly sample positions\n",
    "        random.seed(config.RANDOM_STATE)\n",
    "        n_y = min(len(y_positions), int(np.sqrt(max_patches_per_image)))\n",
    "        n_x = min(len(x_positions), max_patches_per_image // n_y)\n",
    "        \n",
    "        y_positions = sorted(random.sample(y_positions, n_y))\n",
    "        x_positions = sorted(random.sample(x_positions, n_x))\n",
    "    \n",
    "    for y in y_positions:\n",
    "        for x in x_positions:\n",
    "            patch = img[y:y+patch_size, x:x+patch_size]\n",
    "            if patch.shape == (patch_size, patch_size):  # Ensure full patch size\n",
    "                patches.append(patch)\n",
    "    \n",
    "    return np.stack(patches) if patches else np.empty((0, patch_size, patch_size), dtype=img.dtype)\n",
    "\n",
    "def build_patch_dataset_memory_efficient(docroot: str, max_docs: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"Build patch dataset with memory management\"\"\"\n",
    "    by_scanner = group_by_scanner(docroot)\n",
    "    labels = sorted(by_scanner.keys())\n",
    "    \n",
    "    all_patches = []\n",
    "    all_labels = []\n",
    "    patch_count = 0\n",
    "    \n",
    "    for scanner_idx, scanner_name in enumerate(labels):\n",
    "        files = by_scanner[scanner_name][:max_docs] if max_docs else by_scanner[scanner_name]\n",
    "        logger.info(f\"Extracting patches from {len(files)} documents for scanner {scanner_name}\")\n",
    "        \n",
    "        for i, path in enumerate(files):\n",
    "            if patch_count >= config.MAX_PATCHES_IN_MEMORY:\n",
    "                logger.warning(f\"Reached maximum patch limit ({config.MAX_PATCHES_IN_MEMORY}), stopping...\")\n",
    "                break\n",
    "                \n",
    "            img = load_image_gray(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            img_norm = normalize_image(img)\n",
    "            if img_norm is None:\n",
    "                continue\n",
    "                \n",
    "            residual = residual_wavelet(img_norm)\n",
    "            if residual is None:\n",
    "                continue\n",
    "            \n",
    "            patches = extract_patches_from_image(residual)\n",
    "            \n",
    "            if len(patches) > 0:\n",
    "                # Limit patches per image to prevent memory issues\n",
    "                max_patches_per_img = min(len(patches), 50)\n",
    "                if len(patches) > max_patches_per_img:\n",
    "                    indices = np.random.choice(len(patches), max_patches_per_img, replace=False)\n",
    "                    patches = patches[indices]\n",
    "                \n",
    "                all_patches.append(patches)\n",
    "                all_labels.extend([scanner_idx] * len(patches))\n",
    "                patch_count += len(patches)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                logger.info(f\"  Processed {i+1}/{len(files)} files, {patch_count} patches so far\")\n",
    "        \n",
    "        if patch_count >= config.MAX_PATCHES_IN_MEMORY:\n",
    "            break\n",
    "    \n",
    "    if not all_patches:\n",
    "        logger.error(\"No patches extracted!\")\n",
    "        return np.array([]), np.array([]), []\n",
    "    \n",
    "    # Concatenate all patches\n",
    "    X_patches = np.concatenate(all_patches, axis=0)\n",
    "    y_patches = np.array(all_labels, dtype=np.int64)\n",
    "    \n",
    "    logger.info(f\"Patch dataset: {X_patches.shape} patches from {len(labels)} scanners\")\n",
    "    return X_patches, y_patches, labels\n",
    "\n",
    "def train_cnn_model(patches: np.ndarray, labels: np.ndarray, n_classes: int, \n",
    "                   epochs: int = None, batch_size: int = None, lr: float = None) -> ResNetScanner:\n",
    "    \"\"\"Train CNN model with improved training loop\"\"\"\n",
    "    if epochs is None:\n",
    "        epochs = config.CNN_EPOCHS\n",
    "    if batch_size is None:\n",
    "        batch_size = config.CNN_BATCH_SIZE\n",
    "    if lr is None:\n",
    "        lr = config.CNN_LEARNING_RATE\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Training CNN on device: {device}\")\n",
    "    \n",
    "    # Create dataset and split\n",
    "    dataset = PatchDataset(patches, labels)\n",
    "    \n",
    "    # Stratified split for better validation\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(config.RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ResNetScanner(n_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(config.ARTIFACTS_DIR, 'best_cnn_model.pt'))\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                   f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                   f\"Val Acc: {val_acc:.4f}, \"\n",
    "                   f\"LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.ARTIFACTS_DIR, 'training_curves.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_cnn_final(model: ResNetScanner, patches: np.ndarray, labels: np.ndarray, \n",
    "                      label_names: List[str]) -> float:\n",
    "    \"\"\"Final evaluation of CNN model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = PatchDataset(patches, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.CNN_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(pred)\n",
    "            all_targets.extend(target.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    print(f\"\\n=== Final CNN Evaluation ===\")\n",
    "    print(f\"Overall Patch Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(all_targets, all_preds, target_names=label_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ConfusionMatrixDisplay(cm, display_labels=label_names).plot(xticks_rotation=45)\n",
    "    plt.title('CNN Final Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.ARTIFACTS_DIR, 'cnn_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# ==== SAVE/LOAD FUNCTIONS ====\n",
    "def save_models_and_data(models_dict: Dict, fingerprints: Dict, labels: List[str]):\n",
    "    \"\"\"Save all trained models and metadata\"\"\"\n",
    "    # Save traditional ML models\n",
    "    for model_name, model in models_dict.items():\n",
    "        if model is not None:\n",
    "            filename = os.path.join(config.ARTIFACTS_DIR, f\"{model_name.lower().replace(' ', '_')}.joblib\")\n",
    "            joblib.dump({'model': model, 'labels': labels}, filename)\n",
    "            logger.info(f\"Saved {model_name} to {filename}\")\n",
    "    \n",
    "    # Save fingerprints and metadata\n",
    "    metadata = {\n",
    "        'fingerprints': fingerprints,\n",
    "        'labels': labels,\n",
    "        'config': {\n",
    "            'patch_size': config.PATCH_SIZE,\n",
    "            'patch_stride': config.PATCH_STRIDE,\n",
    "            'fft_bins': config.FFT_BINS,\n",
    "            'random_state': config.RANDOM_STATE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_file = os.path.join(config.ARTIFACTS_DIR, 'metadata.joblib')\n",
    "    joblib.dump(metadata, metadata_file)\n",
    "    logger.info(f\"Saved metadata to {metadata_file}\")\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    try:\n",
    "        data = joblib.load(model_path)\n",
    "        return data['model'], data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from {model_path}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ==== MAIN PIPELINE EXECUTION ====\n",
    "def run_complete_pipeline():\n",
    "    \"\"\"Run the complete scanner identification pipeline\"\"\"\n",
    "    print(\"ðŸ” Starting Scanner Identification Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Check paths\n",
    "        check_paths()\n",
    "        \n",
    "        # Step 2: Build scanner fingerprints\n",
    "        print(\"\\nðŸ“‹ Step 1: Building Scanner Fingerprints\")\n",
    "        fingerprints = build_scanner_fingerprints(config.FLATROOT)\n",
    "        \n",
    "        if not fingerprints:\n",
    "            logger.error(\"No fingerprints could be built. Check your flatfield data.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"âœ“ Built fingerprints for {len(fingerprints)} scanners\")\n",
    "        \n",
    "        # Step 3: Build feature-based dataset\n",
    "        print(\"\\nðŸ“Š Step 2: Building Feature Dataset\")\n",
    "        print(\"Processing Official documents...\")\n",
    "        X_official, y_official, labels = build_document_dataset(\n",
    "            config.OFFICIALROOT, fingerprints, config.MAX_DOCS_PER_SCANNER\n",
    "        )\n",
    "        \n",
    "        print(\"Processing Wikipedia documents...\")\n",
    "        X_wiki, y_wiki, _ = build_document_dataset(\n",
    "            config.WIKIROOT, fingerprints, config.MAX_DOCS_PER_SCANNER\n",
    "        )\n",
    "        \n",
    "        if len(X_official) == 0 and len(X_wiki) == 0:\n",
    "            logger.error(\"No feature data could be built. Check your document data.\")\n",
    "            return\n",
    "        \n",
    "        # Combine datasets\n",
    "        X_combined = np.concatenate([X_official, X_wiki], axis=0) if len(X_official) > 0 and len(X_wiki) > 0 else (X_official if len(X_official) > 0 else X_wiki)\n",
    "        y_combined = np.concatenate([y_official, y_wiki], axis=0) if len(y_official) > 0 and len(y_wiki) > 0 else (y_official if len(y_official) > 0 else y_wiki)\n",
    "        \n",
    "        print(f\"âœ“ Combined dataset: {X_combined.shape[0]} samples, {len(labels)} classes\")\n",
    "        \n",
    "        # Step 4: Train traditional ML models\n",
    "        print(\"\\nðŸ¤– Step 3: Training Traditional ML Models\")\n",
    "        models = {}\n",
    "        \n",
    "        # SVM\n",
    "        svm_model = create_svm_pipeline()\n",
    "        models['SVM'] = train_and_evaluate_model(X_combined, y_combined, labels, \"SVM\", svm_model)\n",
    "        \n",
    "        # Random Forest\n",
    "        models['Random Forest'] = train_random_forest(X_combined, y_combined, labels)\n",
    "        \n",
    "        # XGBoost (if available)\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            models['XGBoost'] = train_xgboost(X_combined, y_combined, labels)\n",
    "        \n",
    "        # Step 5: Build patch dataset for CNN\n",
    "        print(\"\\nðŸ§  Step 4: Building Patch Dataset for CNN\")\n",
    "        print(\"Processing Official documents for patches...\")\n",
    "        X_patches_off, y_patches_off, patch_labels = build_patch_dataset_memory_efficient(\n",
    "            config.OFFICIALROOT, config.MAX_DOCS_PER_SCANNER\n",
    "        )\n",
    "        \n",
    "        print(\"Processing Wikipedia documents for patches...\")\n",
    "        X_patches_wiki, y_patches_wiki, _ = build_patch_dataset_memory_efficient(\n",
    "            config.WIKIROOT, config.MAX_DOCS_PER_SCANNER\n",
    "        )\n",
    "        \n",
    "        if len(X_patches_off) == 0 and len(X_patches_wiki) == 0:\n",
    "            logger.warning(\"No patch data could be built. Skipping CNN training.\")\n",
    "            cnn_model = None\n",
    "        else:\n",
    "            # Combine patch datasets\n",
    "            X_patches = np.concatenate([X_patches_off, X_patches_wiki], axis=0) if len(X_patches_off) > 0 and len(X_patches_wiki) > 0 else (X_patches_off if len(X_patches_off) > 0 else X_patches_wiki)\n",
    "            y_patches = np.concatenate([y_patches_off, y_patches_wiki], axis=0) if len(y_patches_off) > 0 and len(y_patches_wiki) > 0 else (y_patches_off if len(y_patches_off) > 0 else y_patches_wiki)\n",
    "            \n",
    "            print(f\"âœ“ Patch dataset: {X_patches.shape[0]} patches\")\n",
    "            \n",
    "            # Step 6: Train CNN\n",
    "            print(\"\\nðŸ”¥ Step 5: Training CNN (ResNet18)\")\n",
    "            cnn_model = train_cnn_model(X_patches, y_patches, len(labels))\n",
    "            \n",
    "            # Final CNN evaluation\n",
    "            evaluate_cnn_final(cnn_model, X_patches, y_patches, labels)\n",
    "        \n",
    "        # Step 7: Save everything\n",
    "        print(\"\\nðŸ’¾ Step 6: Saving Models and Results\")\n",
    "        save_models_and_data(models, fingerprints, labels)\n",
    "        \n",
    "        if cnn_model is not None:\n",
    "            # CNN model is already saved during training (best model)\n",
    "            logger.info(\"CNN model saved as best_cnn_model.pt\")\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ Pipeline completed successfully!\")\n",
    "        print(f\"ðŸ“ All artifacts saved in: {config.ARTIFACTS_DIR}/\")\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"  - Scanner fingerprints: {len(fingerprints)} scanners\")\n",
    "        print(f\"  - Feature dataset: {X_combined.shape[0] if 'X_combined' in locals() else 0} samples\")\n",
    "        print(f\"  - Patch dataset: {X_patches.shape[0] if 'X_patches' in locals() else 0} patches\")\n",
    "        print(f\"  - Trained models: {len([m for m in models.values() if m is not None]) + (1 if cnn_model else 0)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ==== INFERENCE FUNCTIONS ====\n",
    "def predict_document_scanner(image_path: str, model_path: str, fingerprints_path: str) -> Dict:\n",
    "    \"\"\"Predict scanner for a single document\"\"\"\n",
    "    try:\n",
    "        # Load model and metadata\n",
    "        model, labels = load_model(model_path)\n",
    "        metadata = joblib.load(fingerprints_path)\n",
    "        fingerprints = metadata['fingerprints']\n",
    "        \n",
    "        if model is None:\n",
    "            return {\"error\": \"Failed to load model\"}\n",
    "        \n",
    "        # Process image\n",
    "        img = load_image_gray(image_path)\n",
    "        if img is None:\n",
    "            return {\"error\": \"Failed to load image\"}\n",
    "        \n",
    "        img_norm = normalize_image(img)\n",
    "        residual = residual_wavelet(img_norm)\n",
    "        \n",
    "        # Extract features\n",
    "        corr_feats = correlation_features(residual, fingerprints, labels)\n",
    "        fft_feats = fft_radial_stats(residual)\n",
    "        features = np.concatenate([corr_feats, fft_feats]).reshape(1, -1)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(features)[0]\n",
    "        probabilities = model.predict_proba(features)[0] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        result = {\n",
    "            \"predicted_scanner\": labels[prediction],\n",
    "            \"confidence\": float(probabilities[prediction]) if probabilities is not None else None,\n",
    "            \"all_probabilities\": {labels[i]: float(prob) for i, prob in enumerate(probabilities)} if probabilities is not None else None\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def batch_predict_documents(image_folder: str, model_path: str, fingerprints_path: str) -> List[Dict]:\n",
    "    \"\"\"Predict scanner for multiple documents\"\"\"\n",
    "    image_paths = list_images(image_folder)\n",
    "    results = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        result = predict_document_scanner(img_path, model_path, fingerprints_path)\n",
    "        result['image_path'] = img_path\n",
    "        results.append(result)\n",
    "        \n",
    "        if len(results) % 10 == 0:\n",
    "            logger.info(f\"Processed {len(results)}/{len(image_paths)} images\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==== JUPYTER NOTEBOOK EXECUTION ====\n",
    "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
    "    # This section runs when executed in Jupyter notebook\n",
    "    print(\"ðŸ” Scanner Identification Pipeline - Improved Version\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"To run the complete pipeline, execute:\")\n",
    "    print(\">>> run_complete_pipeline()\")\n",
    "    print()\n",
    "    print(\"To predict a single document:\")\n",
    "    print(\">>> result = predict_document_scanner('path/to/image.jpg', 'artifacts/svm.joblib', 'artifacts/metadata.joblib')\")\n",
    "    print()\n",
    "    print(\"To run batch predictions:\")\n",
    "    print(\">>> results = batch_predict_documents('path/to/images/', 'artifacts/svm.joblib', 'artifacts/metadata.joblib')\")\n",
    "    print()\n",
    "    print(\"Configuration can be modified in the Config class above.\")\n",
    "    print(\"Current settings:\")\n",
    "    print(f\"  - Max documents per scanner: {config.MAX_DOCS_PER_SCANNER}\")\n",
    "    print(f\"  - Patch size: {config.PATCH_SIZE}\")\n",
    "    print(f\"  - CNN epochs: {config.CNN_EPOCHS}\")\n",
    "    print(f\"  - Max patches in memory: {config.MAX_PATCHES_IN_MEMORY}\")\n",
    "\n",
    "# Uncomment the line below to run the pipeline automatically\n",
    "# run_complete_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
